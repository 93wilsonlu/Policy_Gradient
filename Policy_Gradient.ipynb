{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy_Gradient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTNC5qKgbZwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f32e433-0ddf-4d7c-c940-088cdf0e5814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import gym\n",
        "import scipy.signal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eps = np.finfo(np.float32).eps.item()\n",
        "max_steps = 4000\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "observation_dimensions = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "seed = 543"
      ],
      "metadata": {
        "id": "r5_LRqnZV6s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c64866d-d7c3-47cc-dbe1-5f5ec368a94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/content/drive/My Drive/Colab Notebooks/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def discounted_cumulative_sums(x, discount):\n",
        "    '''\n",
        "    :param array(n) x: reward\n",
        "    :param float discount\n",
        "    '''\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
      ],
      "metadata": {
        "id": "HrfmBZoydVC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer:\n",
        "    def __init__(self, observation_dimensions, buffer_size, gamma=0.99):\n",
        "        self.observation_buffer = np.zeros(\n",
        "            (buffer_size, observation_dimensions), dtype=np.float32\n",
        "        )\n",
        "        self.action_buffer = np.zeros(buffer_size, dtype=np.int32)\n",
        "        self.reward_buffer = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.advantage_buffer = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.gamma = gamma\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward):\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self):\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
        "            self.reward_buffer[path_slice], self.gamma\n",
        "        )\n",
        "        advantage_mean, advantage_std = self.advantage_buffer[path_slice].mean(), self.advantage_buffer[path_slice].std()\n",
        "        self.advantage_buffer[path_slice] = (self.advantage_buffer[path_slice] - advantage_mean) / (advantage_std + eps)\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        return (\n",
        "            self.observation_buffer[:self.trajectory_start_index],\n",
        "            self.action_buffer[:self.trajectory_start_index],\n",
        "            self.advantage_buffer[:self.trajectory_start_index],\n",
        "        )\n",
        "    \n",
        "    def clear(self):\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "buffer = Buffer(observation_dimensions, max_steps)"
      ],
      "metadata": {
        "id": "xmFk0sJSd96V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, sizes, activation='relu', output_activation=None):\n",
        "    for size in sizes[:-1]:\n",
        "        x = layers.Dense(units=size, activation=activation, kernel_initializer=keras.initializers.HeUniform(), bias_initializer=keras.initializers.HeUniform())(x)\n",
        "        x = layers.Dropout(0.6)(x)\n",
        "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)"
      ],
      "metadata": {
        "id": "6_tca08cVZPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logprobabilities(logits, actions):\n",
        "    '''\n",
        "    :param array(n, num_actions) logits: model output\n",
        "    :param array(n) actions\n",
        "    :return array(n) logprobability\n",
        "    '''\n",
        "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
        "    # logprobabilities_all = tf.nn.softmax(logits)\n",
        "    logprobability = tf.reduce_sum(\n",
        "        tf.one_hot(actions, num_actions) * logprobabilities_all, axis=1\n",
        "    )\n",
        "    return logprobability"
      ],
      "metadata": {
        "id": "2UdWXHKdVwJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    '''\n",
        "    :param array(4) observation\n",
        "    :return int action\n",
        "    '''\n",
        "    logits = model(tf.expand_dims(observation, axis=0))\n",
        "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
        "    return action[0]"
      ],
      "metadata": {
        "id": "q0R-GqgxV5B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(observation_buffer, action_buffer, advantage_buffer, n_trajectory):\n",
        "    '''\n",
        "    :param array(n, observation_dimensions) observation_buffer\n",
        "    :param array(n) action_buffer\n",
        "    :param array(n) advantage_buffer\n",
        "    '''\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = -tf.reduce_sum(\n",
        "            logprobabilities(model(observation_buffer), action_buffer) * advantage_buffer\n",
        "        ) / n_trajectory\n",
        "    \n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return -loss"
      ],
      "metadata": {
        "id": "4yUdh7D2YSBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=1000, n_trajectory_per_epoch=3, max_perfect_count=10000):\n",
        "    env.seed(seed=seed)\n",
        "    mean_reward = np.zeros(epochs)\n",
        "    cnt = 0\n",
        "    for epoch in range(epochs):\n",
        "        observation = env.reset()\n",
        "        buffer.clear()\n",
        "        sum_reward = 0\n",
        "        n_trajectory = 0\n",
        "\n",
        "        while n_trajectory < n_trajectory_per_epoch:\n",
        "            action = int(sample_action(observation))\n",
        "            observation_new, reward, done, _ = env.step(action)\n",
        "            sum_reward += reward\n",
        "\n",
        "            buffer.store(observation, action, reward)\n",
        "\n",
        "            observation = observation_new\n",
        "\n",
        "            if done:\n",
        "                n_trajectory += 1\n",
        "                buffer.finish_trajectory()\n",
        "                observation = env.reset()\n",
        "\n",
        "        (\n",
        "            observation_buffer,\n",
        "            action_buffer,\n",
        "            advantage_buffer,\n",
        "        ) = buffer.get()\n",
        "\n",
        "        mean_reward[epoch] = sum_reward / n_trajectory\n",
        "        if epoch % 10 == 0:\n",
        "            print(\n",
        "                f\"Epoch: {epoch}. Mean Reward: {mean_reward[epoch]}\"\n",
        "            )\n",
        "\n",
        "        if mean_reward[epoch] > env.spec.reward_threshold:\n",
        "            cnt += 1\n",
        "            if cnt > max_perfect_count:\n",
        "                break\n",
        "\n",
        "        loss = train_model(observation_buffer, action_buffer, advantage_buffer, n_trajectory)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Loss: {loss.numpy()}\")\n",
        "        \n",
        "    return mean_reward"
      ],
      "metadata": {
        "id": "w8EEzupm32eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 1000\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(seed=seed)\n",
        "tf.random.set_seed(seed=seed)\n",
        "\n",
        "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
        "logits = mlp(observation_input, [128, num_actions])\n",
        "model = keras.Model(inputs=observation_input, outputs=logits)\n",
        "\n",
        "optimizer = keras.optimizers.Adam()\n",
        "\n",
        "mean_reward = train(n_trajectory_per_epoch=3)"
      ],
      "metadata": {
        "id": "4eBRo8nIWxIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.plot(np.arange(0, epoch), mean_reward[:epoch], label='One step per epoch')\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=20) \n",
        "plt.ylabel('Mean Reward', fontsize=20)\n",
        "plt.title(\"Training Result\", fontsize=20)\n",
        "\n",
        "plt.ylim(0, 510)\n",
        "plt.yticks([0, np.max(mean_reward), env.spec.reward_threshold])\n",
        "plt.grid(axis='y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EHpDjPRO_45A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(env):\n",
        "    current_state = env.reset()\n",
        "    sum_reward = 0\n",
        "    while 1:\n",
        "        action = int(sample_action(current_state))\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        sum_reward += reward\n",
        "        current_state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return sum_reward"
      ],
      "metadata": {
        "id": "4oxGcDYKZcc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(env)"
      ],
      "metadata": {
        "id": "21XpkGwj3MYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers import RecordVideo\n",
        "record_env = RecordVideo(env, './video')\n",
        "print(evaluate(record_env))\n",
        "record_env.close()"
      ],
      "metadata": {
        "id": "PFLkKzku00h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybpMRePt9Xzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}